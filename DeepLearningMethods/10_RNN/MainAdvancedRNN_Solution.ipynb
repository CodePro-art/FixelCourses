{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "concrete-plenty",
   "metadata": {},
   "source": [
    "![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-mattress",
   "metadata": {},
   "source": [
    "# <center> Deep Learning Methods </center>\n",
    "## <center> Lecture 10 - RNN </center>\n",
    "### <center> Advanced RNN </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-sentence",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/DeepLearningMethods/10_RNN/MainAdvancedRNN_Solution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acting-northwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#-- Wide screen:\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "german-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Auto reload:\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "auburn-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn    as nn\n",
    "import torch.optim as optim\n",
    "import torchsummary\n",
    "import torchtext\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab      import Vocab\n",
    "from torchtext.datasets   import IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-cartoon",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "* Use the advanced units and get above 85% accuracy on the sentiment analysis task.  \n",
    "* (Keep all sequences with less than 150 words.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "violent-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirPath             = '../../data'\n",
    "oTokenizer          = get_tokenizer('basic_english')\n",
    "\n",
    "trainIter, testIter = IMDB(root=dirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "satellite-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumWords(line):\n",
    "    return len(line.split())\n",
    "\n",
    "maxLength = 150\n",
    "lTrainSet = [(label, line) for (label, line) in trainIter if NumWords(line) < maxLength]\n",
    "lTestSet  = [(label, line) for (label, line) in testIter  if NumWords(line) < maxLength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "automotive-marsh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000, 9746, 9932)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainIter), len(testIter), len(lTrainSet), len(lTestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "continuing-brunei",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================\n",
      "=========================================================================================================\n",
      "\u001b[32mOften laugh out loud, sometimes sad story of 2 working divorced guys -- Lemmon a neurotic clean \"house husband\" and Matthau a slob sportswriter -- who decide to live together to cut down on expenses. <br /><br />Nicely photographed and directed. The script is very barbed -- that is, there's always more than one side to almost every line. Particularly funny scene involves 2 british sisters (Evans and Shelley) who seem amused by everything anyone says, but when Lemmon busts out his photos of kids and, yes, ex-wife-to-be, he has the girls sobbing along with him before Matthau can show up with the promised drinks!<br /><br />Very entertaining.\u001b[0m\n",
      "--Tokenized:---------------------------\n",
      "['often', 'laugh', 'out', 'loud', ',', 'sometimes', 'sad', 'story', 'of', '2', 'working', 'divorced', 'guys', '--', 'lemmon', 'a', 'neurotic', 'clean', 'house', 'husband', 'and', 'matthau', 'a', 'slob', 'sportswriter', '--', 'who', 'decide', 'to', 'live', 'together', 'to', 'cut', 'down', 'on', 'expenses', '.', 'nicely', 'photographed', 'and', 'directed', '.', 'the', 'script', 'is', 'very', 'barbed', '--', 'that', 'is', ',', 'there', \"'\", 's', 'always', 'more', 'than', 'one', 'side', 'to', 'almost', 'every', 'line', '.', 'particularly', 'funny', 'scene', 'involves', '2', 'british', 'sisters', '(', 'evans', 'and', 'shelley', ')', 'who', 'seem', 'amused', 'by', 'everything', 'anyone', 'says', ',', 'but', 'when', 'lemmon', 'busts', 'out', 'his', 'photos', 'of', 'kids', 'and', ',', 'yes', ',', 'ex-wife-to-be', ',', 'he', 'has', 'the', 'girls', 'sobbing', 'along', 'with', 'him', 'before', 'matthau', 'can', 'show', 'up', 'with', 'the', 'promised', 'drinks', '!', 'very', 'entertaining', '.']\n",
      "=========================================================================================================\n",
      "=========================================================================================================\n",
      "\u001b[31mThe 3-D featured in \"The Man Who Wasn't There\" stands for DUMB, DUMB, DUMB! This inept comedy features lousy 3-D effects that makes the 3-D effects in \"Jaws 3\", \"Amityville 3\", and \"Friday the 13th Part 3\" look better by comparison. Not to mention the movie is asinine to the extreme. This was one of many 1983 movies to feature the pop-off-the-screen effects. Steve Guttenberg and Jeffrey Tambor got trapped in this mess, but at least it didn't kill their careers. Tambor would go on to star on HBO's \"The Larry Sanders Show\" and Ron Howard's box office smash \"How the Grinch Stole Christmas\", while Guttenberg followed this flop with \"Police Academy\" and \"Cocoon\". What them in those projects instead of them here in \"The Man Who Wasn't There\". If you do, you'll regret it.<br /><br />1/2* (out of four)\u001b[0m\n",
      "--Tokenized:---------------------------\n",
      "['the', '3-d', 'featured', 'in', 'the', 'man', 'who', 'wasn', \"'\", 't', 'there', 'stands', 'for', 'dumb', ',', 'dumb', ',', 'dumb', '!', 'this', 'inept', 'comedy', 'features', 'lousy', '3-d', 'effects', 'that', 'makes', 'the', '3-d', 'effects', 'in', 'jaws', '3', ',', 'amityville', '3', ',', 'and', 'friday', 'the', '13th', 'part', '3', 'look', 'better', 'by', 'comparison', '.', 'not', 'to', 'mention', 'the', 'movie', 'is', 'asinine', 'to', 'the', 'extreme', '.', 'this', 'was', 'one', 'of', 'many', '1983', 'movies', 'to', 'feature', 'the', 'pop-off-the-screen', 'effects', '.', 'steve', 'guttenberg', 'and', 'jeffrey', 'tambor', 'got', 'trapped', 'in', 'this', 'mess', ',', 'but', 'at', 'least', 'it', 'didn', \"'\", 't', 'kill', 'their', 'careers', '.', 'tambor', 'would', 'go', 'on', 'to', 'star', 'on', 'hbo', \"'\", 's', 'the', 'larry', 'sanders', 'show', 'and', 'ron', 'howard', \"'\", 's', 'box', 'office', 'smash', 'how', 'the', 'grinch', 'stole', 'christmas', ',', 'while', 'guttenberg', 'followed', 'this', 'flop', 'with', 'police', 'academy', 'and', 'cocoon', '.', 'what', 'them', 'in', 'those', 'projects', 'instead', 'of', 'them', 'here', 'in', 'the', 'man', 'who', 'wasn', \"'\", 't', 'there', '.', 'if', 'you', 'do', ',', 'you', \"'\", 'll', 'regret', 'it', '.', '1/2*', '(', 'out', 'of', 'four', ')']\n",
      "=========================================================================================================\n",
      "=========================================================================================================\n",
      "\u001b[31mThis is not the worst film I have seen of Peter Greenaway but it is close. That dishonor goes to the even worse Pillow Book. This director's films of 3 I have seen I find them all to be miserable. Like The Cook...,whatever positive cinematic flourishes he displays, are totally unredeemed by the repugnancy of his material and overall presentation.\u001b[0m\n",
      "--Tokenized:---------------------------\n",
      "['this', 'is', 'not', 'the', 'worst', 'film', 'i', 'have', 'seen', 'of', 'peter', 'greenaway', 'but', 'it', 'is', 'close', '.', 'that', 'dishonor', 'goes', 'to', 'the', 'even', 'worse', 'pillow', 'book', '.', 'this', 'director', \"'\", 's', 'films', 'of', '3', 'i', 'have', 'seen', 'i', 'find', 'them', 'all', 'to', 'be', 'miserable', '.', 'like', 'the', 'cook', '.', '.', '.', ',', 'whatever', 'positive', 'cinematic', 'flourishes', 'he', 'displays', ',', 'are', 'totally', 'unredeemed', 'by', 'the', 'repugnancy', 'of', 'his', 'material', 'and', 'overall', 'presentation', '.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "red   = '\\x1b[31m'\n",
    "green = '\\x1b[32m'\n",
    "end   = '\\x1b[0m'\n",
    "\n",
    "for _  in range(3):\n",
    "    (label, line) = random.choice(lTrainSet)\n",
    "    \n",
    "    color = red if label == 'neg' else green\n",
    "    print('=========================================================================================================')\n",
    "    print('=========================================================================================================')\n",
    "    print(color + line + end)\n",
    "    print('--Tokenized:---------------------------')    \n",
    "    print(oTokenizer(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "prospective-organization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.Vocab at 0x23572dd8df0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "oCounter  = Counter()\n",
    "for ii, (label, line) in enumerate(lTrainSet):\n",
    "    oCounter.update(oTokenizer(line))\n",
    "    \n",
    "oVocab = Vocab(oCounter, min_freq=10, specials=('<unk>', '<BOS>', '<EOS>', '<pad>'))\n",
    "oVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "presidential-translator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6321"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "vietnamese-plaintiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : Hello World!\n",
      "Output: tensor([   1, 3630,  212,   25,    2])\n"
     ]
    }
   ],
   "source": [
    "def TextTransform(line):\n",
    "    return torch.tensor([oVocab['<BOS>']] + [oVocab[token] for token in oTokenizer(line)] + [oVocab['<EOS>']])\n",
    "\n",
    "def LabelTransform(label):\n",
    "    return 1 if label == 'pos' else 0\n",
    "\n",
    "# Print out the output of text_transform\n",
    "line = 'Hello World!'\n",
    "print('Input :', line)\n",
    "print('Output:', TextTransform(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "practical-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "def CollateBatch(lBatch):\n",
    "    Nb  = len(lBatch)\n",
    "    vY  = torch.zeros(Nb)\n",
    "    lX  = [None] * Nb\n",
    "    for ii, (label, line) in enumerate(lBatch):\n",
    "        vY[ii] = LabelTransform(label)\n",
    "        lX[ii] = TextTransform (line)\n",
    "    \n",
    "    mPackX = pack_sequence(lX, enforce_sorted=False)\n",
    "\n",
    "    return mPackX, vY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "authorized-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 128\n",
    "oTrainDL  = DataLoader(lTrainSet, batch_size=batchSize,   shuffle=True, collate_fn=CollateBatch)\n",
    "oTestDL   = DataLoader(lTestSet,  batch_size=2*batchSize, shuffle=True, collate_fn=CollateBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "regional-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "def PackedAs(mX, mPack):\n",
    "    return PackedSequence(mX, mPack.batch_sizes, None, mPack.unsorted_indices)\n",
    "\n",
    "V = len(oVocab)\n",
    "D = 24\n",
    "H = 16\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.oEmbedding = nn.Embedding(V,   D, padding_idx=oVocab['<pad>'])\n",
    "        self.oGRU       = nn.GRU      (D,   H, num_layers=1, bidirectional=True)\n",
    "        self.oFC        = nn.Linear   (2*H, 1)\n",
    "        \n",
    "    def forward(self, mPackX):\n",
    "                                                             #-- mPackX.shape = (N*T,)\n",
    "        mE     = self.oEmbedding(mPackX.data)                #-- mE    .shape = (N*T, D)\n",
    "        mPackE = PackedAs       (mE, mPackX)                 #-- mPackE.shape = (N*T, D)\n",
    "        _, mH  = self.oGRU      (mPackE)                     #-- mH    .shape = (2,   N, H)\n",
    "        mH     = torch.cat([mH[-1,:,:], mH[-2,:,:]], dim=1)  #-- mH    .shape = (N,   2*H)\n",
    "        mZ     = self.oFC       (mH)                         #-- mZ    .shape = (N,   1)\n",
    "        return mZ[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "positive-statistics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mX, vY = next(iter(oTrainDL))\n",
    "RNN()(mX).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "apparent-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "Loss   = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-static",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: | Train loss: 0.69528 | Val loss: 0.69378 | Train Metric: 0.4928 | Val Metric: 0.4986 | epoch time:  4.828 | <-- Checkpoint!\n",
      "Epoch 001: | Train loss: 0.69370 | Val loss: 0.69248 | Train Metric: 0.5037 | Val Metric: 0.5124 | epoch time:  4.745 | <-- Checkpoint!\n",
      "Epoch 002: | Train loss: 0.69232 | Val loss: 0.69167 | Train Metric: 0.5182 | Val Metric: 0.5273 | epoch time:  5.197 | <-- Checkpoint!\n",
      "Epoch 003: | Train loss: 0.69110 | Val loss: 0.69044 | Train Metric: 0.5368 | Val Metric: 0.5437 | epoch time:  5.162 | <-- Checkpoint!\n",
      "Epoch 004: | Train loss: 0.69010 | Val loss: 0.68991 | Train Metric: 0.5507 | Val Metric: 0.5537 | epoch time:  5.373 | <-- Checkpoint!\n",
      "Epoch 005: | Train loss: 0.68932 | Val loss: 0.68896 | Train Metric: 0.5624 | Val Metric: 0.5724 | epoch time:  5.643 | <-- Checkpoint!\n",
      "Epoch 006: | Train loss: 0.68751 | Val loss: 0.68635 | Train Metric: 0.5840 | Val Metric: 0.5876 | epoch time:  6.075 | <-- Checkpoint!\n",
      "Epoch 007: | Train loss: 0.67989 | Val loss: 0.67240 | Train Metric: 0.6120 | Val Metric: 0.6313 | epoch time:  7.006 | <-- Checkpoint!\n",
      "Epoch 008: | Train loss: 0.60498 | Val loss: 0.53084 | Train Metric: 0.7206 | Val Metric: 0.7803 | epoch time:  6.538 | <-- Checkpoint!\n",
      "Epoch 009: | Train loss: 0.44731 | Val loss: 0.46057 | Train Metric: 0.8440 | Val Metric: 0.8210 | epoch time:  6.088 | <-- Checkpoint!\n",
      "Epoch 010: | Train loss: 0.39272 | Val loss: 0.47731 | Train Metric: 0.8785 | Val Metric: 0.8310 | epoch time:  5.934 | <-- Checkpoint!\n",
      "Epoch 011: | Train loss: 0.40862 | Val loss: 0.38638 | Train Metric: 0.8845 | Val Metric: 0.8690 | epoch time:  6.034 | <-- Checkpoint!\n",
      "Epoch 012: | Train loss: 0.35368 | Val loss: 0.42072 | Train Metric: 0.9050 | Val Metric: 0.8418 | epoch time:  6.503 |\n",
      "Epoch 013: | Train loss: 0.34920 | Val loss: 0.44033 | Train Metric: 0.9157 | Val Metric: 0.8247 | epoch time:  6.133 |\n",
      "Epoch 014: | Train loss: 0.34297 | Val loss: 0.41025 | Train Metric: 0.9186 | Val Metric: 0.8434 | epoch time:  5.913 |\n",
      "Epoch 015: | Train loss: 0.32935 | Val loss: 0.42950 | Train Metric: 0.9229 | Val Metric: 0.8282 | epoch time:  5.674 |\n",
      "Epoch 016: | Train loss: 0.33950 | Val loss: 0.36897 | Train Metric: 0.9181 | Val Metric: 0.8827 | epoch time:  5.708 | <-- Checkpoint!\n",
      "Epoch 017: | Train loss: 0.31531 | Val loss: 0.42216 | Train Metric: 0.9242 | Val Metric: 0.8274 | epoch time:  5.675 |\n",
      "Epoch 018: | Train loss: 0.32687 | Val loss: 0.41269 | Train Metric: 0.9235 | Val Metric: 0.8491 | epoch time:  5.769 |\n",
      "Epoch 019: | Train loss: 0.31496 | Val loss: 0.40778 | Train Metric: 0.9205 | Val Metric: 0.8329 | epoch time:  5.793 |\n",
      "Epoch 020: | Train loss: 0.31933 | Val loss: 0.41505 | Train Metric: 0.9220 | Val Metric: 0.8471 | epoch time:  5.547 |\n",
      "Epoch 021: | Train loss: 0.34173 | Val loss: 0.37162 | Train Metric: 0.9187 | Val Metric: 0.8742 | epoch time:  5.601 |\n",
      "Epoch 022: | Train loss: 0.31104 | Val loss: 0.41717 | Train Metric: 0.9225 | Val Metric: 0.8216 | epoch time:  5.736 |\n",
      "Epoch 023: | Train loss: 0.31348 | Val loss: 0.36057 | Train Metric: 0.9244 | Val Metric: 0.8870 | epoch time:  5.711 | <-- Checkpoint!\n",
      "Epoch 024: | Train loss: 0.31084 | Val loss: 0.36957 | Train Metric: 0.9282 | Val Metric: 0.8757 | epoch time:  5.657 |\n",
      "Epoch 025: | Train loss: 0.29340 | Val loss: 0.38928 | Train Metric: 0.9306 | Val Metric: 0.8597 | epoch time:  5.723 |\n",
      "Epoch 026: | Train loss: 0.31452 | Val loss: 0.39577 | Train Metric: 0.9288 | Val Metric: 0.8501 | epoch time:  5.634 |\n",
      "Epoch 027: | Train loss: 0.30892 | Val loss: 0.36199 | Train Metric: 0.9287 | Val Metric: 0.8849 | epoch time:  5.828 |\n",
      "Epoch 028: | Train loss: 0.28923 | Val loss: 0.39065 | Train Metric: 0.9360 | Val Metric: 0.8455 | epoch time:  6.041 |\n",
      "Epoch 029: | Train loss: 0.28491 | Val loss: 0.40469 | Train Metric: 0.9402 | Val Metric: 0.8316 | epoch time:  5.865 |\n",
      "Epoch 030: | Train loss: 0.27388 | Val loss: 0.40235 | Train Metric: 0.9434 | Val Metric: 0.8377 | epoch time:  6.097 |\n",
      "Epoch 031: | Train loss: 0.27978 | Val loss: 0.34744 | Train Metric: 0.9444 | Val Metric: 0.8854 | epoch time:  6.037 |\n",
      "Epoch 032: | Train loss: 0.25651 | Val loss: 0.34416 | Train Metric: 0.9522 | Val Metric: 0.8816 | epoch time:  5.964 |\n",
      "Epoch 033: | Train loss: 0.25715 | Val loss: 0.35728 | Train Metric: 0.9521 | Val Metric: 0.8761 | epoch time:  6.192 |\n",
      "Epoch 034: | Train loss: 0.27196 | Val loss: 0.36540 | Train Metric: 0.9528 | Val Metric: 0.8762 | epoch time:  5.497 |\n",
      "Epoch 035: | Train loss: 0.24922 | Val loss: 0.33605 | Train Metric: 0.9560 | Val Metric: 0.8821 | epoch time:  5.615 |\n",
      "Epoch 036: | Train loss: 0.22559 | Val loss: 0.35113 | Train Metric: 0.9635 | Val Metric: 0.8773 | epoch time:  5.530 |\n",
      "Epoch 037: | Train loss: 0.22445 | Val loss: 0.33743 | Train Metric: 0.9646 | Val Metric: 0.8814 | epoch time:  5.647 |\n",
      "Epoch 038: | Train loss: 0.22916 | Val loss: 0.35049 | Train Metric: 0.9678 | Val Metric: 0.8788 | epoch time:  5.539 |\n",
      "Epoch 039: | Train loss: 0.21736 | Val loss: 0.34109 | Train Metric: 0.9727 | Val Metric: 0.8736 | epoch time:  5.724 |\n",
      "Epoch 040: | Train loss: 0.20546 | Val loss: 0.34248 | Train Metric: 0.9750 | Val Metric: 0.8774 | epoch time:  5.971 |\n",
      "Epoch 041: | Train loss: 0.20282 | Val loss: 0.33940 | Train Metric: 0.9755 | Val Metric: 0.8775 | epoch time:  5.652 |\n",
      "Epoch 042: | Train loss: 0.19100 | Val loss: 0.34227 | Train Metric: 0.9774 | Val Metric: 0.8729 | epoch time:  5.602 |\n",
      "Epoch 043: | Train loss: 0.18837 | Val loss: 0.34089 | Train Metric: 0.9794 | Val Metric: 0.8729 | epoch time:  5.594 |\n",
      "Epoch 044: | Train loss: 0.18455 | Val loss: 0.34627 | Train Metric: 0.9811 | Val Metric: 0.8707 | epoch time:  5.794 |\n",
      "Epoch 045: | Train loss: 0.18001 | Val loss: 0.34844 | Train Metric: 0.9814 | Val Metric: 0.8669 | epoch time:  5.586 |\n",
      "Epoch 046: | Train loss: 0.17612 | Val loss: 0.33834 | Train Metric: 0.9821 | Val Metric: 0.8716 | epoch time:  5.658 |\n",
      "Val - Iteration:   4 (39): loss = 0.31658089"
     ]
    }
   ],
   "source": [
    "from torch.optim                    import lr_scheduler\n",
    "from DeepLearningFramework.Metric   import BinaryAcuuracy\n",
    "from DeepLearningFramework.Training import TrainClassficationModel\n",
    "\n",
    "nEpochs       = 50\n",
    "nIter         = nEpochs * len(oTrainDL)\n",
    "oModel        = RNN().to(DEVICE)\n",
    "oOptim        = optim.AdamW(oModel.parameters(), lr=2e-5, betas=(0.8, 0.9), weight_decay=5)\n",
    "oScheduler    = lr_scheduler.OneCycleLR(oOptim, max_lr=2e-3, total_steps=nIter)\n",
    "lHistory      = TrainClassficationModel(oModel, oTrainDL, oTestDL, Loss, BinaryAcuuracy, nEpochs, oOptim, oScheduler);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepLearningFramework.Auxiliary import PlotHistory\n",
    "\n",
    "PlotHistory(lHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-battery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-crystal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
