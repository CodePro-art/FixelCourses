{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Machine Learning Methods </center>\n",
    "## <center> Lecture 25 - Introduction to NLP</center>\n",
    "### <center> Wrod2Vec </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethod/25_NLP/MainWord2Vec.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rc('font', **{'size' : 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB Sentiment Analysis:\n",
    "https://www.kaggle.com/kaushik3497/imdb-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>3453_3</td>\n",
       "      <td>0</td>\n",
       "      <td>It seems like more consideration has gone into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>5064_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't believe they made this film. Completel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>10905_3</td>\n",
       "      <td>0</td>\n",
       "      <td>Guy is a loser. Can't get girls, needs to buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>10194_3</td>\n",
       "      <td>0</td>\n",
       "      <td>This 30 minute documentary Buñuel made in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>8478_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I saw this movie as a child and it broke my he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sentiment                                             review\n",
       "0       5814_8          1  With all this stuff going down at the moment w...\n",
       "1       2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2       7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3       3630_4          0  It must be assumed that those who praised this...\n",
       "4       9495_8          1  Superbly trashy and wondrously unpretentious 8...\n",
       "...        ...        ...                                                ...\n",
       "24995   3453_3          0  It seems like more consideration has gone into...\n",
       "24996   5064_1          0  I don't believe they made this film. Completel...\n",
       "24997  10905_3          0  Guy is a loser. Can't get girls, needs to buil...\n",
       "24998  10194_3          0  This 30 minute documentary Buñuel made in the ...\n",
       "24999   8478_8          1  I saw this movie as a child and it broke my he...\n",
       "\n",
       "[25000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N     = 25000\n",
    "dData = pd.read_csv('labeledTrainData.tsv', delimiter='\\t')\n",
    "dData = dData[:N]\n",
    "dData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing:\n",
    "* Remove HTML stuff.\n",
    "* Remove punctuation and switch to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from   bs4 import BeautifulSoup  \n",
    "\n",
    "def PreprocessLine(text):\n",
    "    \n",
    "    #-- Remove <br> and HTML:\n",
    "    text   = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    #-- Keep lower case letters:\n",
    "    lWords = re.sub(\"[^a-zA-Z]\", \" \", text).lower().split() \n",
    "    \n",
    "    return  \" \".join(lWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from   nltk.corpus import stopwords\n",
    "from   nltk.stem   import WordNetLemmatizer\n",
    "from   bs4         import BeautifulSoup  \n",
    "\n",
    "oWordNetLemmatizer = WordNetLemmatizer()\n",
    "sStopWords         = set(stopwords.words('english')) \n",
    "\n",
    "def PreprocessLine(text, printFlag=False):\n",
    "        \n",
    "    if printFlag == False:\n",
    "        print2 = lambda str: None\n",
    "    else:\n",
    "        print2 = lambda str: print(str)\n",
    "        \n",
    "    print2('Original text:')\n",
    "    print2(text)\n",
    "    print2('----------------------------------------------------------\\n')\n",
    "    \n",
    "    print2('Remove <br> and HTML:')\n",
    "    \n",
    "    print2(text)\n",
    "    print2('----------------------------------------------------------\\n')\n",
    "    \n",
    "    print2('Keep lower case letters:')\n",
    "    lWords = re.sub(\"[^a-zA-Z]\", \" \", text).lower().split() \n",
    "    print2(lWords)\n",
    "    print2('----------------------------------------------------------\\n')\n",
    "    \n",
    "#     print2('Lemmatization:')\n",
    "#     lWords = [oWordNetLemmatizer.lemmatize(word) for word in lWords if word not in sStopWords]   \n",
    "#     print2(lWords)\n",
    "#     print2('----------------------------------------------------------\\n')\n",
    "    \n",
    "    return  lWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text into lines:\n",
    "Using `tokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<br /><br />This movie is full of references. Like \\Mad Max II\\\", \\\"The wild one\\\" and many others. The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre. This movie is a masterpiece. We´ll talk much more about in the future.\"\n",
      "=================\n",
      "=================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<br /><br />This movie is full of references.',\n",
       " 'Like \\\\Mad Max II\\\\\", \\\\\"The wild one\\\\\" and many others.',\n",
       " 'The ladybug´s face it´s a clear reference (or tribute) to Peter Lorre.',\n",
       " 'This movie is a masterpiece.',\n",
       " 'We´ll talk much more about in the future.\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "sText  = dData['review'].values[9]\n",
    "lLines = tokenizer.tokenize(sText.strip())\n",
    "\n",
    "print(sText)\n",
    "print('=================')\n",
    "print('=================')\n",
    "lLines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert raw text to processed lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Text2lines(text):\n",
    "    lRawLines = tokenizer.tokenize(text.strip())\n",
    "    lLines = []\n",
    "    \n",
    "    for rawLine in lRawLines:\n",
    "        if len(rawLine) > 0:\n",
    "            lLines.append(PreprocessLine(rawLine))\n",
    "\n",
    "    return lLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lLines = []\n",
    "for text in dData['review'].values:\n",
    "    lLines += Text2lines(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec model:\n",
    "Using `gensim`  \n",
    "https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "d            = 300\n",
    "minWordCount = 40\n",
    "contextWin   = 5\n",
    "\n",
    "oWord2Vec = word2vec.Word2Vec(lLines, workers=4, size=d, min_count=minWordCount, window=contextWin)\n",
    "\n",
    "#-- If you don't plan to train the model any further, calling \n",
    "#-- init_sims will make the model much more memory-efficient.\n",
    "oWord2Vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8308"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oWord2Vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which word from the given list doesn't go with the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Or\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'film'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oWord2Vec.wv.doesnt_match(['man', 'child', 'woman', 'film'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 0.8880027532577515),\n",
       " ('documentary', 0.6742621064186096),\n",
       " ('picture', 0.6466796398162842),\n",
       " ('flick', 0.6288743615150452),\n",
       " ('sequel', 0.5803209543228149),\n",
       " ('masterpiece', 0.5269690155982971),\n",
       " ('films', 0.5204336047172546),\n",
       " ('segment', 0.5121288895606995),\n",
       " ('thriller', 0.5106791853904724),\n",
       " ('cinema', 0.5068612098693848)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oWord2Vec.wv.most_similar('film')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single word embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'this'\n",
    "vZ   = oWord2Vec.wv[word]\n",
    "vZ.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algebra with words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.8203716278076172)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vWord = oWord2Vec.wv['actor'] - oWord2Vec.wv['man'] +  oWord2Vec.wv['woman']\n",
    "oWord2Vec.wv.most_similar(positive=[vWord], topn=1)\n",
    "\n",
    "# oWord2Vec.wv.most_similar(positive=['actor', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model words embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8308, 300), (8308, 300))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-- Embedding vectors (Vw)\n",
    "mZ = oWord2Vec.wv.vectors\n",
    "#-- Context vectors (Vc)\n",
    "mC = oWord2Vec.trainables.syn1neg\n",
    "\n",
    "mZ.shape, mC.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute $K$ clusters and print most similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K       = 10\n",
    "oKmeans = KMeans(n_clusters=K, n_init=1, init='random').fit(mZ)\n",
    "mMu     = oKmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Cluster 0:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ask', 0.6485928297042847),\n",
       " ('say', 0.6430670022964478),\n",
       " ('guess', 0.6388207674026489),\n",
       " ('leave', 0.6332195997238159),\n",
       " ('warn', 0.6293051838874817),\n",
       " ('forgive', 0.6214397549629211),\n",
       " ('buy', 0.6196361780166626),\n",
       " ('think', 0.6162622570991516),\n",
       " ('understand', 0.6160597205162048),\n",
       " ('regret', 0.614203929901123)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('climbing', 0.8881481289863586),\n",
       " ('racing', 0.8640392422676086),\n",
       " ('spaceship', 0.8573546409606934),\n",
       " ('mountain', 0.8503150939941406),\n",
       " ('laser', 0.8500832915306091),\n",
       " ('crashes', 0.8498013019561768),\n",
       " ('ranch', 0.8490526080131531),\n",
       " ('flames', 0.8454511165618896),\n",
       " ('windows', 0.8430398106575012),\n",
       " ('yard', 0.8426832556724548)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 2:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('natives', 0.823610782623291),\n",
       " ('groups', 0.8109252452850342),\n",
       " ('methods', 0.8036380410194397),\n",
       " ('inmates', 0.8035577535629272),\n",
       " ('muslims', 0.8012620806694031),\n",
       " ('cities', 0.7992398738861084),\n",
       " ('workers', 0.7922746539115906),\n",
       " ('companies', 0.7908176183700562),\n",
       " ('dancers', 0.7901692390441895),\n",
       " ('slaves', 0.787493109703064)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 3:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('imagery', 0.8432190418243408),\n",
       " ('aesthetic', 0.8067528009414673),\n",
       " ('storytelling', 0.8001387119293213),\n",
       " ('blend', 0.7867540121078491),\n",
       " ('atmosphere', 0.7765041589736938),\n",
       " ('artistry', 0.7763543128967285),\n",
       " ('scope', 0.7749865055084229),\n",
       " ('flair', 0.7732428312301636),\n",
       " ('technique', 0.7716243267059326),\n",
       " ('symbolism', 0.758178174495697)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 4:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('unsettling', 0.8582496643066406),\n",
       " ('improbable', 0.8398882150650024),\n",
       " ('preposterous', 0.8398847579956055),\n",
       " ('unpredictable', 0.8349109888076782),\n",
       " ('restrained', 0.8341395854949951),\n",
       " ('stylized', 0.8336154818534851),\n",
       " ('simplistic', 0.82525634765625),\n",
       " ('clumsy', 0.8245313167572021),\n",
       " ('vivid', 0.8210886120796204),\n",
       " ('poetic', 0.8150763511657715)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 5:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('policeman', 0.897533118724823),\n",
       " ('dictator', 0.8954358100891113),\n",
       " ('millionaire', 0.8869168758392334),\n",
       " ('dealer', 0.8862613439559937),\n",
       " ('salesman', 0.8700532913208008),\n",
       " ('abu', 0.8655052185058594),\n",
       " ('playboy', 0.8649786114692688),\n",
       " ('investigator', 0.8608105778694153),\n",
       " ('attorney', 0.8553116917610168),\n",
       " ('preacher', 0.8551937341690063)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 6:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ideals', 0.8482915163040161),\n",
       " ('corruption', 0.8396477699279785),\n",
       " ('turmoil', 0.8337985277175903),\n",
       " ('destruction', 0.8320393562316895),\n",
       " ('beliefs', 0.8309690952301025),\n",
       " ('addiction', 0.8285077214241028),\n",
       " ('dilemma', 0.8259705305099487),\n",
       " ('awareness', 0.8209913969039917),\n",
       " ('plight', 0.8209081292152405),\n",
       " ('involvement', 0.8199225664138794)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 7:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('columbia', 0.8638345003128052),\n",
       " ('august', 0.8076130151748657),\n",
       " ('wwe', 0.7895396947860718),\n",
       " ('revival', 0.7722963690757751),\n",
       " ('uk', 0.7620391845703125),\n",
       " ('region', 0.7544271349906921),\n",
       " ('corporation', 0.7538974285125732),\n",
       " ('fifties', 0.7523468732833862),\n",
       " ('boogeyman', 0.7453266978263855),\n",
       " ('code', 0.7446240782737732)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 8:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('everett', 0.9420100450515747),\n",
       " ('keith', 0.9394093155860901),\n",
       " ('mitchell', 0.932623565196991),\n",
       " ('thomas', 0.930459201335907),\n",
       " ('rooney', 0.9293650388717651),\n",
       " ('miller', 0.9288190603256226),\n",
       " ('stevens', 0.9278692007064819),\n",
       " ('collins', 0.9263511896133423),\n",
       " ('burke', 0.9262815713882446),\n",
       " ('bennett', 0.9243419170379639)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Cluster 9:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('informs', 0.8278855085372925),\n",
       " ('reveals', 0.7913388609886169),\n",
       " ('commits', 0.7677836418151855),\n",
       " ('blames', 0.7613875269889832),\n",
       " ('considers', 0.7564082741737366),\n",
       " ('admits', 0.7492868900299072),\n",
       " ('forgets', 0.7487760782241821),\n",
       " ('convinces', 0.7476508021354675),\n",
       " ('demonstrates', 0.7459062337875366),\n",
       " ('approached', 0.7453361749649048)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ii in range(K):\n",
    "    print('--------------------------')\n",
    "    print(f'Cluster {ii}:')\n",
    "    display(oWord2Vec.wv.similar_by_vector(mMu[ii,:]))\n",
    "    print('--------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 546.604818,
   "position": {
    "height": "40px",
    "left": "1423.25px",
    "right": "20px",
    "top": "115.966px",
    "width": "333.679px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
