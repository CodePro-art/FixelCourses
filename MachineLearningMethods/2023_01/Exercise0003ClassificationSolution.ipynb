{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Fixel Algorithms](https://fixelalgorithms.co/images/CCExt.png)](https://fixelalgorithms.gitlab.io/)\n",
    "\n",
    "# Machine Learning Methods\n",
    "\n",
    "## Exercise 003 - Classification\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 13/02/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/MachineLearningMethods/2023_01/Exercise0002ClassificationSolution.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T09:30:06.492269Z",
     "start_time": "2022-02-02T09:30:06.220934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "# General Tools\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Miscellaneous\n",
    "import json\n",
    "import os\n",
    "from platform import python_version\n",
    "import random\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bokeh.plotting import figure, show\n",
    "\n",
    "# Jupyter\n",
    "from IPython import get_ipython\n",
    "from IPython.display import Image, display\n",
    "from ipywidgets import Dropdown, FloatSlider, interact, IntSlider, Layout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# %matplotlib inline\n",
    "\n",
    "seedNum = 512\n",
    "np.random.seed(seedNum)\n",
    "random.seed(seedNum)\n",
    "\n",
    "# sns.set_theme() #>! Apply SeaBorn theme\n",
    "\n",
    "runInGoogleColab = 'google.colab' in str(get_ipython())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "FIG_SIZE_DEF = (8, 8)\n",
    "ELM_SIZE_DEF = 50\n",
    "CLASS_COLOR = ('b', 'r')\n",
    "EDGE_COLOR  = 'k'\n",
    "\n",
    "TEST_DATA_FILE_NAME  = 'TestData.mat'\n",
    "TRAIN_DATA_FILE_NAME = 'TrainData.mat'\n",
    "\n",
    "L_CLASSES   = ['Red', 'Green', 'Blue']\n",
    "IMG_SIZE    = [100, 100]\n",
    "\n",
    "DATA_FILE_URL  = r'https://github.com/FixelAlgorithmsTeam/FixelCourses/raw/master/DataSets/KaggleWhatsCooking.json'\n",
    "DATA_FILE_NAME = 'KaggleWhatsCooking.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixel Algorithms Packages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "This exercise introduces:\n",
    "\n",
    " - Working with real world data in the context of basic Natural Language Processing (NLP).\n",
    " - Working with binary features using Decision Trees.\n",
    " - Working with Ensemble Method based on trees.\n",
    " - Utilizing the `LightGBM` package with the `LGBMClassifier`.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> One of the objectives of this exercise is working on non trivial data set in size, features and performance.\n",
    "\n",
    "In this exercise we'll work the data set: [Yummly - What's Cooking?](https://www.kaggle.com/competitions/whats-cooking) from [Kaggle](https://www.kaggle.com).  \n",
    "The data set is basically a list of ingredients of a recipe (Features) and the type of cuisine of the recipe (Italian, French, Indian, etc...).  \n",
    "The objective is being able to classify the cuisine of a recipe by its ingredients.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The data set will be downloaded and parsed automatically.\n",
    "\n",
    "The data will be defined as the following:\n",
    "\n",
    "1. A boolean matrix of size `numSamples x numFeatures`.\n",
    "2. The features are the list of all ingredients in the recipes.\n",
    "3. For a recipe, the features vector is hot encoding of the features.  \n",
    "\n",
    "For example, if the list of features is: `basil, chicken, egg, eggplant, garlic, pasta, salt, tomato sauce`.  \n",
    "Then for Pasta with Tomato Sauce the features vector will be: `[1, 0, 0, 0, 1, 1, 1, 1]` which means: `basil, garlic, pasta, salt, tomato sauce`.\n",
    "This will be the basic feature list while you're encourages to add more features.\n",
    "\n",
    "In this exercise:\n",
    "\n",
    "1. Download the data (Will be done automatically by the code).\n",
    "2. Parse data into a data structure to work with (Automatically by the code).\n",
    "3. Extract features from the recipes (The basic features: Existence of an ingredient).\n",
    "4. Train an Ensemble of Decision Trees using the LightGBM models (Very fast).\n",
    "5. Optimize the model hyper parameters (See below).\n",
    "6. Plot the _confusion matrix_ of the best model on the data.\n",
    "\n",
    "Optimize features (repeat if needed) to get accuracy of at least `70%`.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> It might be useful to use the [NLTK](https://github.com/nltk/nltk) package for [word stemming](https://en.wikipedia.org/wiki/Stemming).\n",
    "* <font color='brown'>(**#**)</font> To install the package (Prior to working with the notebook):\n",
    "  - Open Anaconda command line (`Prompt`).\n",
    "  - Activate the `IAIMLMethods` environment by: `conda activate IAIMLMethods`.\n",
    "  - Install the package using `conda install nltk -c conda-forge`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "numSamplesTrain = 35_000\n",
    "numSamplesTest  = None\n",
    "\n",
    "# Hyper Parameters of the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the list of learning rate (4 values in range [0.05, 0.5]).\n",
    "# 2. Set the list of maximum iterations (3 integer values in range [10, 200]).\n",
    "# 3. Set the list of maximum nodes (3 integer values in range [10, 50]).\n",
    "lLearnRate  = [0.05, 0.10, 0.15, 0.20]\n",
    "lMaxItr     = [50, 100, 200]\n",
    "lMaxNodes   = [20, 30, 50]\n",
    "#===============================================================#\n",
    "\n",
    "numFold     = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='blue'>(**!**)</font> Fill the function **after** reading the code below which use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "def ReadData( filePath: str ) -> tuple[list, list, list]:\n",
    "    # read data into lists\n",
    "    \n",
    "    hFile = open(filePath)\n",
    "    dJsonData = json.load(hFile)\n",
    "        \n",
    "    lId, lCuisine, lIngredients = [], [], []\n",
    "    for ii in range(len(dJsonData)):\n",
    "        lId.append(dJsonData[ii]['id'])\n",
    "        lCuisine.append(dJsonData[ii]['cuisine'])\n",
    "        lIngredients.append(dJsonData[ii]['ingredients'])  \n",
    "                \n",
    "    return lId, lCuisine, lIngredients\n",
    "\n",
    "def RemoveDigits( lIngredients: list ) -> list:\n",
    "    # Remove digits from the ingredients list\n",
    "    \n",
    "    #===========================Fill This===========================#\n",
    "    # 1. Look for the symbol of a digit in RegExp.\n",
    "    return [[re.sub(\"\\d+\", \"\", x) for x in y] for y in lIngredients]\n",
    "    #===============================================================#\n",
    "\n",
    "def RemoveChars( lIngredients: list ) -> list:\n",
    "    # Remove some un required characters from the ingredients list\n",
    "   \n",
    "    lIngredients = [[x.replace(\"-\", \" \") for x in y] for y in lIngredients]\n",
    "    #===========================Fill This===========================# \n",
    "    # 01. Remove the following: & \n",
    "    # 02. Remove the following: '\n",
    "    # 03. Remove the following: ''\n",
    "    # 04. Remove the following: % \n",
    "    # 05. Remove the following: ! \n",
    "    # 06. Remove the following: (  \n",
    "    # 07. Remove the following: ) \n",
    "    # 08. Remove the following: / \n",
    "    # 09. Remove the following: \\ \n",
    "    # 10. Remove the following: , \n",
    "    # 11. Remove the following: . \n",
    "    # 12. Remove the following: \"\n",
    "    lIngredients = [[x.replace(\"&\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"'\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"''\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"%\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"!\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"(\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\")\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"/\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\"\\\\\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\",\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(\".\", \" \") for x in y] for y in lIngredients]\n",
    "    lIngredients = [[x.replace('\"', \" \") for x in y] for y in lIngredients]\n",
    "    #===============================================================# \n",
    "    lIngredients = [[x.replace(u\"\\u2122\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(u\"\\u00AE\", \" \") for x in y] for y in lIngredients] \n",
    "    lIngredients = [[x.replace(u\"\\u2019\", \" \") for x in y] for y in lIngredients] \n",
    "\n",
    "    return lIngredients\n",
    "\n",
    "def LowerCase( lIngredients: list ) -> list:\n",
    "    # Make letters lowercase for the ingredients list\n",
    "    \n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Make lower case of the text. \n",
    "    # !! Pay attention that the input is a list of lists!\n",
    "    return [[x.lower() for x in y] for y in lIngredients]\n",
    "    #===============================================================# \n",
    "\n",
    "def RemoveRedundantWhiteSpace( lIngredients: list ) -> list:\n",
    "    # Removes redundant whitespaces\n",
    "    \n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Make lower case of the text. \n",
    "    # !! Pay attention that the input is a list of lists!\n",
    "    return [[re.sub( '\\s+', ' ', x).strip() for x in y] for y in lIngredients] \n",
    "    #===============================================================# \n",
    "    \n",
    "    \n",
    "def StemWords( lIngredients: list ) -> list:\n",
    "    # Word stemming for ingredients list (Per word)\n",
    "    \n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Construct the `WordNetLemmatizer` object.\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    #===============================================================# \n",
    "    \n",
    "    def WordByWord( inStr: str ):\n",
    "        \n",
    "        return \" \".join([\"\".join(lmtzr.lemmatize(w)) for w in inStr.split()])\n",
    "    \n",
    "    return [[WordByWord(x) for x in y] for y in lIngredients] \n",
    "    \n",
    "    \n",
    "def RemoveUnits( lIngredients: list ) -> list:\n",
    "    # Remove units related words from ingredients\n",
    "    \n",
    "    remove_list = ['g', 'lb', 's', 'n']\n",
    "        \n",
    "    def CheckWord( inStr: str ):\n",
    "        \n",
    "        splitStr = inStr.split()\n",
    "        resStr  = [word for word in splitStr if word.lower() not in remove_list]\n",
    "        \n",
    "        return ' '.join(resStr)\n",
    "\n",
    "    return [[CheckWord(x) for x in y] for y in lIngredients]\n",
    "\n",
    "def ExtractUniqueIngredients( lIngredients: list, sortList: bool = True ) -> list:\n",
    "    # Extract all unique ingredients from the list as a single list\n",
    "\n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Extract the unique values of ingredients (You use the `set()` data type of Python).\n",
    "    # 2. Sort it by name if `sortList == True`. \n",
    "    lUniqueIng = list(set([ing for lIngredient in lIngredients for ing in lIngredient]))\n",
    "    if sortList:\n",
    "        lUniqueIng = sorted(lUniqueIng)\n",
    "    #===============================================================# \n",
    "\n",
    "    return lUniqueIng\n",
    "\n",
    "def ExtractFeatureEncoding( lIngredient: list, lUniqueIng: list ) -> np.ndarray:\n",
    "    # If an ingredient is in the specific recipe\n",
    "    \n",
    "    mF = np.zeros(shape = (len(lIngredient), len(lUniqueIng)), dtype = np.uint)\n",
    "    #===========================Fill This===========================# \n",
    "    # 1. Iterate over the list of lists of the ingredients.\n",
    "    # 2. For each sample (List of ingredients), put 1 in the location of the ingredients.\n",
    "    for ii in range(len(lIngredient)):\n",
    "        for jj in lIngredient[ii]:\n",
    "            mF[ii, lUniqueIng.index(jj)] = 1\n",
    "    #===============================================================# \n",
    "            \n",
    "    return mF\n",
    "\n",
    "def PlotLabelsHistogram(vY: np.ndarray, hA = None, lClass = None, xLabelRot: int = None) -> plt.Axes:\n",
    "\n",
    "    if hA is None:\n",
    "        hF, hA = plt.subplots(figsize = (8, 6))\n",
    "    \n",
    "    vLabels, vCounts = np.unique(vY, return_counts = True)\n",
    "\n",
    "    hA.bar(vLabels, vCounts, width = 0.9, align = 'center')\n",
    "    hA.set_title('Histogram of Classes / Labels')\n",
    "    hA.set_xlabel('Class')\n",
    "    hA.set_ylabel('Number of Samples')\n",
    "    hA.set_xticks(vLabels)\n",
    "    if lClass is not None:\n",
    "        hA.set_xticklabels(lClass)\n",
    "    \n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA\n",
    "\n",
    "def PlotConfusionMatrix(vY: np.ndarray, vYPred: np.ndarray, normMethod: str = None, hA: plt.Axes = None, lLabels: list = None, dScore: dict = None, titleStr: str = 'Confusion Matrix', xLabelRot: int = None, valFormat: str = None) -> plt.Axes:\n",
    "\n",
    "    # Calculation of Confusion Matrix\n",
    "    mConfMat = confusion_matrix(vY, vYPred, normalize = normMethod)\n",
    "    oConfMat = ConfusionMatrixDisplay(mConfMat, display_labels = lLabels)\n",
    "    oConfMat = oConfMat.plot(ax = hA, values_format = valFormat)\n",
    "    hA = oConfMat.ax_\n",
    "    if dScore is not None:\n",
    "        titleStr += ':'\n",
    "        for scoreName, scoreVal in  dScore.items():\n",
    "            titleStr += f' {scoreName} = {scoreVal:0.2},'\n",
    "        titleStr = titleStr[:-1]\n",
    "    hA.set_title(titleStr)\n",
    "    hA.grid(False)\n",
    "    if xLabelRot is not None:\n",
    "        for xLabel in hA.get_xticklabels():\n",
    "            xLabel.set_rotation(xLabelRot)\n",
    "\n",
    "    return hA, mConfMat\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate / Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "# This section downloads data from the given URL if needed.\n",
    "\n",
    "if not os.path.exists(DATA_FILE_NAME):\n",
    "    urllib.request.urlretrieve(DATA_FILE_URL, DATA_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading / Generating Data\n",
    "\n",
    "lId, lCuisine, lIngredients = ReadData(DATA_FILE_NAME)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing the Data\n",
    "\n",
    "In this section we'll do as following:\n",
    "\n",
    "1. Make all text _lower case_.\n",
    "2. Remove digits (Weights etc...).\n",
    "3. Remove some not needed chars.\n",
    "4. Remove redundant spaces.\n",
    "5. Remove units.\n",
    "6. Stem the text (See [Word Stemming](https://en.wikipedia.org/wiki/Stemming)).\n",
    "\n",
    "The objective is to reduce the sensitivity to the style used to describe the ingredients.  \n",
    "So we're after the most basic way to describe each ingredient.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The list above is the minimum to be done. You may use more ideas.\n",
    "* <font color='brown'>(**#**)</font> Look at the features list after this. You'll find more duplications and improvements ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Fill the body of the function above.\n",
    "lIng = LowerCase(lIngredients)\n",
    "lIng = RemoveDigits(lIng)\n",
    "lIng = RemoveChars(lIng)\n",
    "lIng = RemoveRedundantWhiteSpace(lIng)\n",
    "lIng = RemoveUnits(lIng)\n",
    "lIng = StemWords(lIng)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Features\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Fill the body of the function above.\n",
    "lFeat = ExtractUniqueIngredients(lIng)\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> The function `` matches based on teh whole name of the ingredient. For multi words ingredients one might use even a match of a single word.  \n",
    "This can be useful for cases like `ketchup` vs. `tomato ketchup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Features Encoding\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Fill the body of the function above.\n",
    "mF = ExtractFeatureEncoding(lIng, lFeat)\n",
    "#===============================================================#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Data\n",
    "# Create a Data Frame of the data\n",
    "dfX = pd.DataFrame(columns = lFeat, data = mF)\n",
    "dfX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Labels Data\n",
    "\n",
    "dsY = pd.Series(data = lCuisine, name = 'Cuisine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels as Categorical Data\n",
    "\n",
    "vY          = pd.Categorical(dsY).codes\n",
    "lEncoding   = pd.Categorical(dsY).categories.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hF, hA = plt.subplots(figsize = (12, 8))\n",
    "hA = PlotLabelsHistogram(dsY, hA = hA, xLabelRot = 90)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Is this a balanced data set?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "\n",
    "We'll split the data into training and testing.  \n",
    "Set `numSamplesTrain`. For teh first tries you use small number just to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train & Test Data\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Split the data using `train_test_split()`.\n",
    "# 2. Make sure to use `numSamplesTrain` and `numSamplesTest`.\n",
    "# 3. Set the `random_state` so iterative runs will be reproducible.\n",
    "mXTrain, mXTest, vYTrain, vYTest = train_test_split(mF, vY, train_size = numSamplesTrain, test_size = numSamplesTest, random_state = seedNum, shuffle = True, stratify = vY)\n",
    "#===============================================================#\n",
    "\n",
    "\n",
    "# Dimensions of the Data\n",
    "print(f'The number of training data samples: {mXTrain.shape[0]}')\n",
    "print(f'The number of training features per sample: {mXTrain.shape[1]}') \n",
    "\n",
    "\n",
    "print(f'The number of test data samples: {mXTest.shape[0]}')\n",
    "print(f'The number of test features per sample: {mXTest.shape[1]}') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> What's the ratio of the train samples vs/ number of features? What do you think it should be?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Classes\n",
    "\n",
    "# Train\n",
    "hA = PlotLabelsHistogram(vYTrain, lClass = lEncoding, xLabelRot = 90)\n",
    "hA.set_title(hA.get_title() + ' - Train Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of Classes\n",
    "\n",
    "# Test\n",
    "hA = PlotLabelsHistogram(vYTest, lClass = lEncoding, xLabelRot = 90)\n",
    "hA.set_title(hA.get_title() + ' - Test Data')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Which score method would you use between _accuracy_, _recall_, _precision_ or _f1_?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data and Feature Engineering / Extraction\n",
    "\n",
    "The vector of values doesn't fit, as is, for classification with SVM.  \n",
    "It misses a lot of the information given in the structure of the image or a color pixel.  \n",
    "In our case, the important thing is to give the classifier information about the structure of color, a vector of 3 values: `[r, g, b]`.  \n",
    "Yet, the classifier input is limited to a list of values. This is where the concept of metric comes into play.  \n",
    "\n",
    "We need to create information about distance between colors.  \n",
    "We also need to extract features to represent the colors in the image.\n",
    "\n",
    "In this section the task are:\n",
    "\n",
    "1. Implement functions to extract features from the data.\n",
    "2. Arrange the features in a _matrix_ / _data frame_ for processing.\n",
    "3. Explore the features using _SeaBorn_. Specifically if the features extracts meaningful information.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> Don't include _test data_ in the analysis for feature extraction. Other wise, a data leakage will happen.\n",
    "\n",
    "### Ideas for Features\n",
    "\n",
    "1. The distance between the the _mean_ / _median_ / _mode_ color of the image to the per _mean_ / _median_ / _mode_ color per class.\n",
    "2. The distance between the quantized histogram of `R` / `G` / `B` color channels of the image to the class.\n",
    "3. The distance of the mean color at the center of the image to the mean color of the class.\n",
    "4. The channel with the maximum value (Is this a continuous value? Does it fit the SVM model?).\n",
    "5. Use of the _HSL_ color space.\n",
    "\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You're encouraged to think on more features!\n",
    "* <font color='brown'>(**#**)</font> Pay attention to dimensionality fo the data. For instance, how do you define the _median color_?\n",
    "* <font color='brown'>(**#**)</font> For simplicity we use the RGB Color Space. Yet color distance might be better calculated in other color spaces (See LAB for instance)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Classifiers\n",
    "\n",
    "In this section we'll train a Kernel SVM model with optimized hyper parameters: `C` and `gamma`.  \n",
    "The score should be the regular accuracy.\n",
    "\n",
    "1. Build the dictionary of parameters for the grid search.\n",
    "2. Construct the grid search object (`GridSearchCV`).\n",
    "3. Optimize the hyper parameters by the `fit()` method of the grid search object.\n",
    "\n",
    "* <font color='red'>(**?**)</font> Why is the accuracy a reasonable score in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Construct the Grid Search Object \n",
    "\n",
    "# #===========================Fill This===========================#\n",
    "# # 1. Set the parameters to iterate over and their values.\n",
    "# dParams = {'learning_rate': lLearnRate, 'max_iter': lMaxItr, 'max_leaf_nodes': lMaxNodes}\n",
    "# #===============================================================#\n",
    "\n",
    "# vCatFeatFlag = np.full(shape = len(lFeat), fill_value = True)\n",
    "# oGsSvc = GridSearchCV(estimator = HistGradientBoostingClassifier(categorical_features = vCatFeatFlag), param_grid = dParams, scoring = 'f1_micro', cv = numFold, verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimize\n",
    "\n",
    "# #===========================Fill This===========================#\n",
    "# # 1. Apply the grid search phase.\n",
    "# oGsSvc = oGsSvc.fit(mXTrain, vYTrain)\n",
    "# #===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the Grid Search Object \n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Set the parameters to iterate over and their values.\n",
    "dParams = {'num_leaves': lMaxNodes, 'learning_rate': lLearnRate, 'n_estimators': lMaxItr}\n",
    "#===============================================================#\n",
    "\n",
    "vCatFeatFlag = np.full(shape = len(lFeat), fill_value = True)\n",
    "oGsSvc = GridSearchCV(estimator = LGBMClassifier(), param_grid = dParams, scoring = 'f1_micro', cv = numFold, verbose = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Apply the grid search phase.\n",
    "oGsSvc = oGsSvc.fit(mXTrain, vYTrain)\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='brown'>(**#**)</font> One could optimize the histogram by creating a 3D histogram."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Analysis\n",
    "\n",
    "In this section the relation between the features and the labels is analyzed.  \n",
    "You should visualize / calculate measures which imply the features makes the classes identifiable.\n",
    "\n",
    "#### Ideas for Analysis\n",
    "\n",
    "1. Display the histogram / density of each feature by the label of sample.\n",
    "2. Display the correlation between the feature to the class value (Pay attention this is a mix of continuous values and categorical values).\n",
    "\n",
    "* <font color='brown'>(**#**)</font> You may find SeaBorn's `kdeplot()` useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix on Test Data \n",
    "\n",
    "In this section we'll test the model on the test data.\n",
    "\n",
    "1. Extract the best estimator from the grid search.\n",
    "2. If needed, fit it to the train data.\n",
    "3. Calculate the test set features. Make sure to avoid data leakage.\n",
    "4. Display the _confusion matrix_.\n",
    "\n",
    "The objective is to get at least `85%` accuracy per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Best Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Get the best model with the optimized hyper parameters.\n",
    "bestModel = oGsSvc.best_estimator_\n",
    "#===============================================================#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> Does the best model need a refit on data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the Model\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "# 1. Train the model on the whole training data\n",
    "bestModel = bestModel.fit(mXTrain, vYTrain)\n",
    "#===============================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix\n",
    "hF, hA = plt.subplots(figsize = (12, 12))\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "hA, mConfMat = PlotConfusionMatrix(vYTrain, bestModel.predict(mXTrain), lLabels = lEncoding, hA = hA, xLabelRot = 90, normMethod = 'true', valFormat = '0.0%')\n",
    "hA.set_title(hA.get_title() + ' - Train Data')\n",
    "#===============================================================#\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Confusion Matrix\n",
    "hF, hA = plt.subplots(figsize = (12, 12))\n",
    "\n",
    "#===========================Fill This===========================#\n",
    "hA, mConfMat = PlotConfusionMatrix(vYTest, bestModel.predict(mXTest), lLabels = lEncoding, hA = hA, xLabelRot = 90, normMethod = 'true', valFormat = '0.0%')\n",
    "hA.set_title(hA.get_title() + ' - Test Data')\n",
    "#===============================================================#\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color='red'>(**?**)</font> How would you handle the case the test would have features not in the training?\n",
    "* <font color='green'>(**@**)</font> Try to get more features and improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
