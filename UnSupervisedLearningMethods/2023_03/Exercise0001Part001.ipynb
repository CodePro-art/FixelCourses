{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/qkg2E2D.png)\n",
    "\n",
    "# UnSupervised Learning Methods\n",
    "\n",
    "## Exercise 001 - Part I\n",
    "\n",
    "> Notebook by:\n",
    "> - Royi Avital RoyiAvital@fixelalgorithms.com\n",
    "\n",
    "## Revision History\n",
    "\n",
    "| Version | Date       | User        |Content / Changes                                                   |\n",
    "|---------|------------|-------------|--------------------------------------------------------------------|\n",
    "| 0.1.000 | 12/03/2023 | Royi Avital | First version                                                      |\n",
    "|         |            |             |                                                                    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FixelAlgorithmsTeam/FixelCourses/blob/master/UnSupervisedLearningMethods/2023_03/0004EstimationNonParametric.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations\n",
    "\n",
    "* <font color='red'>(**?**)</font> Question to answer interactively.\n",
    "* <font color='blue'>(**!**)</font> Simple task to add code for the notebook.\n",
    "* <font color='green'>(**@**)</font> Optional / Extra self practice.\n",
    "* <font color='brown'>(**#**)</font> Note / Useful resource / Food for thought."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    " - Answer all questions within the Jupyter Notebook.\n",
    " - Open questions are in part I of the exercise.\n",
    " - Coding based questions are in the subsequent notebooks.\n",
    " - Use MarkDown + MathJaX + Code to answer.\n",
    " - Submission in groups (Single submission per group).\n",
    " - You may and _should_ use the forums for question.\n",
    " - Good Luck!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convexity\n",
    "\n",
    "**Convex Set**  \n",
    "\n",
    "Let:\n",
    "\n",
    "$$ \\mathbb{R}_{\\geq 0}^{d} = \\left\\{ \\boldsymbol{x} \\in\\mathbb{R}^{d} \\, \\bigg| \\, \\min_{i} {x}_{i} \\geq 0 \\right\\} $$\n",
    "\n",
    "Where $\\boldsymbol{x} = \\begin{bmatrix} {x}_{1} \\\\ {x}_{2} \\\\ \\vdots \\\\ {x}_{d} \\end{bmatrix}$\n",
    "\n",
    "### 1.1. Question\n",
    "\n",
    "Prove or disprove that $\\mathbb{R}_{\\geq 0}^{d}$ is convex."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convex Combination** \n",
    "\n",
    "Let $\\mathcal{C} \\subseteq \\mathbb{R}^{d} $ be a convex set and consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\} _{i=1}^{N}$.\n",
    "\n",
    "### 1.2. Question\n",
    "\n",
    "Prove that for any $N \\in \\mathbb{N}$: \n",
    "\n",
    "$$ \\sum_{i = 1}^{N} {\\alpha}_{i} \\boldsymbol{x}_{i} \\in \\mathcal{C} $$\n",
    "\n",
    "Where $\\alpha_{i}$ are such that: \n",
    "\n",
    " - $\\forall i, \\; \\alpha_{i} \\geq 0$.\n",
    " - $\\sum_{i = 1}^{N} \\alpha_{i} = 1$.\n",
    "\n",
    "* <font color='brown'>(**#**)</font> The properties of ${\\alpha}_{i}$ above means it is sampled from the Unit Probability Simplex.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{C}\\subset\\mathbb{R}^{2}$ be a convex set.  \n",
    "Consider $\\left\\{ \\boldsymbol{x}_{i} \\in \\mathcal{C} \\right\\}_{i=1}^{10}$ such that $\\boldsymbol{x}_{i} \\neq \\boldsymbol{x}_{j}$ for all $i \\neq j$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Question\n",
    "\n",
    "Prove or disprove the following assertion:\n",
    "\n",
    "Necessarily, any point $\\boldsymbol{y} \\in \\mathcal{C}$ can be represented as a convex combination of $\\left\\{ \\boldsymbol{x}_{i} \\right\\}_{i = 1}^{10}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Gradient\n",
    "\n",
    "**Remark**: Assume all functions in this section are differentiable.\n",
    "\n",
    "\n",
    "**Directional Derivative**\n",
    "\n",
    "Let $f : \\mathbb{R}^{d} \\to \\mathbb{R}$ and let $\\boldsymbol{x}_{0} \\in \\mathbb{R}^{d}$. \n",
    "\n",
    "### 2.1. Question\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$ \\forall \\boldsymbol{h} \\in \\mathbb{R}^{d}: \\nabla f \\left( \\boldsymbol{x}_{0} \\right) \\left[ \\boldsymbol{h} \\right] = \\left\\langle \\boldsymbol{g}_{0}, \\boldsymbol{h} \\right\\rangle \\implies \\boldsymbol{g}_{0} = \\nabla f \\left( \\boldsymbol{x}_{0} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition**\n",
    "\n",
    "$f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ is said to be **linear** if:\n",
    "\n",
    "$$ f \\left( \\alpha \\boldsymbol{x} + \\beta \\boldsymbol{y} \\right) = \\alpha f \\left( \\boldsymbol{x} \\right) + \\beta f \\left( \\boldsymbol{y} \\right) $$\n",
    "\n",
    "For all $\\alpha, \\beta \\in \\mathbb{R}$ and for all $\\boldsymbol{x}, \\boldsymbol{y} \\in \\mathbb{R}^{{d}_{1}}$.\n",
    "\n",
    "\n",
    "\n",
    "Let $f : \\mathbb{R}^{{d}_{1}} \\to \\mathbb{R}^{{d}_{2}}$ be a linear function.\n",
    "\n",
    "### 2.2. Question\n",
    "\n",
    "Prove that:\n",
    "\n",
    "$$ \\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right] = f \\left( \\boldsymbol{h} \\right) $$\n",
    "\n",
    "For all $\\boldsymbol{x}, \\boldsymbol{h} \\in \\mathbb{R}^{{d}_{1}}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{A} \\boldsymbol{X} \\right\\} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {\\left\\| \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x} \\right\\|}_{2}^{2} $$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = {\\left\\| \\boldsymbol{Y} - \\boldsymbol{A} \\boldsymbol{X} \\right\\|}_{F}^{2} $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{D \\times d}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$.\n",
    " - ${\\left\\| \\cdot \\right\\|}_{F}^{2}$ is the squared [Frobenius Norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm), that is, ${\\left\\| \\boldsymbol{X} \\right\\|}_{F}^{2} = \\left\\langle \\boldsymbol{X}, \\boldsymbol{X} \\right\\rangle = \\operatorname{Tr} \\left\\{ \\boldsymbol{X}^{T} \\boldsymbol{X} \\right\\}$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{X}^{T} \\boldsymbol{A}, \\boldsymbol{Y}^{T} \\right\\rangle $$\n",
    "\n",
    "Where $\\boldsymbol{Y} \\in \\mathbb{R}^{D \\times N}$, $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times D}$ and $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times N}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = {a}^{T} g \\left( \\boldsymbol{x} \\right) $$\n",
    "\n",
    "Where $g \\left( \\cdot \\right)$ is an element wise function $g \\left( \\boldsymbol{x} \\right) = \\begin{bmatrix} g \\left( {x}_{1} \\right) \\\\ g \\left( {x}_{2} \\right) \\\\ \\vdots \\\\ g \\left( {x}_{d} \\right) \\end{bmatrix} \\in \\mathbb{R}^{d}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{A}, \\log \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\log \\left( \\cdot \\right)$ is the element wise $\\log$ function: $\\boldsymbol{M} = \\log \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{M} \\left[ i, j \\right] = \\log \\left( \\boldsymbol{X} \\left[ i, j\\right] \\right)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Question\n",
    "\n",
    "Compute the directional derivative $\\nabla f \\left( \\boldsymbol{x} \\right) \\left[ \\boldsymbol{h} \\right]$ and the gradient $\\nabla f \\left( \\boldsymbol{x} \\right)$ of:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{X} \\right) = \\left\\langle \\boldsymbol{a}, \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\right\\rangle $$\n",
    "\n",
    "Where:\n",
    "\n",
    " - $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times d}$.\n",
    " - The function $\\operatorname{Diag} \\left( \\cdot \\right) : \\mathbb{R}^{d \\times d} \\to \\mathbb{R}^{d} $ returns the diagonal of a matrix, that is, $\\boldsymbol{b} = \\operatorname{Diag} \\left( \\boldsymbol{X} \\right) \\implies \\boldsymbol{b} \\left[ i \\right] = \\left( \\boldsymbol{X} \\left[ i, i\\right] \\right)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constraint Optimization\n",
    "\n",
    "**MinMax**  \n",
    "\n",
    "Let $G \\left( x, y \\right) = \\sin \\left( x + y \\right)$.\n",
    "\n",
    "### 3.1. Question\n",
    "\n",
    "Show that:\n",
    "\n",
    " - $\\underset{x}{\\min} \\underset{y}{\\max} G \\left( x, y \\right) = 1$.\n",
    " - $\\underset{y}{\\max} \\underset{x}{\\min} G \\left( x, y \\right) = -1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rayleigh Quotient**  \n",
    "\n",
    "The _Rayleigh Quotient_ is defined by:\n",
    "\n",
    "$$ f \\left( \\boldsymbol{x} \\right) = \\frac{ \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} }{ \\boldsymbol{x}^{T} \\boldsymbol{x}} $$\n",
    "\n",
    "For some symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "### 3.2. Question\n",
    "\n",
    "Follow the given steps:\n",
    "\n",
    " - Show that $ {\\min}_{\\boldsymbol{x}} f \\left( \\boldsymbol{x} \\right) = \\begin{cases} {\\min}_{\\boldsymbol{x}} \\boldsymbol{x}^{T} \\boldsymbol{A} \\boldsymbol{x} \\\\ \\text{ s.t. } {\\left\\| \\boldsymbol{x} \\right\\|}_{2}^{2} = 1 \\end{cases} $.\n",
    " - Write the Lagrangian of the constraint objective $\\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right)$.\n",
    " - Show that ${\\nabla}_{\\boldsymbol{x}} \\mathcal{L} \\left( \\boldsymbol{x}, \\lambda \\right) = 0 \\iff \\boldsymbol{A} \\boldsymbol{x} = \\lambda \\boldsymbol{x}$.  \n",
    "   In other words, the stationary points $\\left( \\boldsymbol{x}, \\lambda \\right)$ are the eigenvectors and eigenvalues of $\\boldsymbol{A}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Solution\n",
    "\n",
    "<font color='red'>??? Fill the answer here ???</font>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/qIP5xPv.png\" height=\"700\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "39577bab1f263e62e0b74f5b8086bd735049bf4751f6562b2d4b2969dc308293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
